
<!DOCTYPE html>
<html>
<head>
    <title>Data Analyst</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Adecco is looking for Data Analyst!</h1>
            <h2>Remote | Full-time | 166 applicants |  Mid-Senior level</h2>
            <h2>Bengaluru, Karnataka</h2>
        </header>
        <main>
            <p id="jobDescription">Additional Information:
Company Profile Our client is the largest solar PV, energy storage system and EV charging solution distributor in the UK. Offering the largest portfolio of products from the worlds leading manufacturers. Operating via it's online portal it offers full access to product information, ordering and stock checking facilities and system design tools 24/7. Plus pre and post-sales support, access to training and bespoke events. Job Title: Data Engineering Lead Reporting to: Head of Data We want to be at the forefront of making a positive impact on the world. Our focus is providing Clean Technology to make the world a more sustainable place and every single transaction we make creates a brighter future. Role summary The Data Engineering Lead is responsible for the design, development and maintenance of robust scalable and high-performance data platform with Snowflake and Fivetran including ETL/ELT and OLAP solutions that address business and technical requirements as defined by the business need. Implementing and driving the adoption for a new Snowflake data warehouse, this role will be accountable for delivery of a truly best-in-class data capability to prepare or significant growth both in terms of revenue and data volumes. Responsibility will include the creation and maintenance of extraction, load and transformation services for the provision of data for analytics, taking an approach of late-binding within data solutions, as well as leveraging the aptitude of the existing team from traditional SQL into new methodologies which maximise agility and value. You will act as principle architect of the end to end solution, supported by strategic third parties who are experts in the field to create an agile and future-proofed data capability which will meet our desire to scale indefinitely and become the best and most agile in the industry.
Responsibilities:• Demonstrate experience architecting and implementing solutions in cloud environments under an agile framework
• Being comfortable defining and maintaining a high standard for development in close collaboration with data analysts, data scientists, product owners, DBAs and external stakeholders of varied seniority
• Use your skills as a critical thinker who is able to anticipate and prevent challenges before they occur with solid design and a careful attention to detail and will be commercial enough to understand the purpose of your output - being able to competently challenge assumptions, business cases and approaches if there could be a better option
• · Responsible for the analysis, design, development, testing and maintenance of ELT solutions, predictive models, tabular models and data marts
• · Ownership for cloud-based data lake, using serverless architectures wherever possible to absolutely minimise the need for platform maintenance and permit exponential growth with diminishing effort
• · Delivery of data governance which informs a single source of truth in respect to information
• · To highly automate the entire SDLC according to best practice patterns and practices which result in a high velocity data engineering service to the business
• · To work according to best practices and patterns which enable us to move at the highest velocity in managing change
• · Helping users adhere to agile practices utilising a toolset that includes Clickup, Azure Dev Ops and continuous integration build systems
• · Establish and maintain a best-in-class approach to testing, leaning heavily towards full automation of unit test, regression test and integration test and leading by example in respect to testing and data quality processes
• · Own and continuously improve development standards for the data engineering team
• This includes coding standards, version control practices, peer reviews, and the creation and versioning of ETL documentation artefacts
• · Perform root cause analysis for troubleshooting production run-time issues
• · To rigorously investigate failed releases, identifying, prioritising and eliminating root causes as a process for continuous improvement
• · Deliver results within a multi-task work environment
• · Work with business SMEs and Business Analysts to convert business needs into technical requirements
• · Ownership of a comprehensive data catalog for all metadata, business rules and logic
• · Create and maintain development packages within agreed SLA
• · Performance tuning data artefacts as well as fundamental code tuning
• · Optimize SQL, Python, R code performance and processing methods
• · Perform design validation, reconciliation and error handling across the entire ELT process (initial, incremental)
• Requirement and skills (Job Technical Skills)· Strong programming knowledge and skills with Snowflake· Strong programming using Python and R
• · Solid experience working in an environment where analytics for data surfaces in Power BI and Excel
• · Strong background Microsoft SQL 2019- our application database which is ingested into Snowflake
• · Demonstrated skill and ability in the development of data warehouse using best practices, including star and snowflake schemas, FACT tables, cubes and tabular models and associated languages such as TSQL
• · Good understanding of OLTP and OLAP databases· Demonstrably excellent understanding of Kimball Data Warehouse concepts in theory and practice· Excellent on building and tuning queries in TSQL· Evidence of heavy involvement in creation of data warehouse from inception to delivery
• · Experience with Agile development practices· Experience with continuous integration and delivery in theory and practice Additional Skills· Databricks, Apache Spark, Hadoop and Hive or another similar platform utilising map-reduce functionality such as EMR
• · Working with Supply chain· A genuine interest in sustainability and a desire to save the planet using technology
• · Presenting data engineering status/projects to senior management Behavioural competencies· Strong written and verbal communication skills
• · Team-player with good interpersonal skills
• · Ability to work productively on your own and remotely for periods of time
• · Critical thinker· Commercial awareness· Strong attention to detail If interested please share your updated cv to the following id: keya
• pramanik@adecco
• coQualifications:• m</p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    