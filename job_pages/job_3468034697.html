
<!DOCTYPE html>
<html>
<head>
    <title>Data Analyst</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Games24x7 is looking for Data Analyst!</h1>
            <h2>On-site | Full-time | 0 applicants |  Mid-Senior level</h2>
            <h2>Bengaluru, Karnataka</h2>
        </header>
        <main>
            <p id="jobDescription"><br>Additional Information:<br>Introduction Games24x7 is India’s leading and most valuable multi-gaming unicorn. We’re a full-stack gaming company, offering awesome game playing experiences to over 100 million players through our products - Rummy Circle, India’s first and largest online rummy platform, My11Circle, the country’s fastest growing fantasy sports platform, and U Games, a cutting-edge gaming studio making casual games in India for players across the globe. A pioneer in the online skill gaming industry in India, Games24x7 was founded in 2006 when two New York University trained economists Bhavin Pandya, and Trivikraman Thampy met at the computer lab and discovered their shared passion for online games. We’ve always been a technology company at heart, and over the last decade and a half, we’ve built the organisation on a strong foundation of ‘the science of gaming’, leveraging behavioural science, artificial intelligence, and machine learning to provide immersive and hyper-personalised gaming experiences to each of our players. Backed by marquee investors including Tiger Global Management, The Raine Group, and Malabar Investment Advisors, Games24x7 is leading the charge in India’s gaming revolution, constantly innovating and offering novel entertainment to players! Our 700 passionate teammates create their magic from our offices in Mumbai, Bengaluru, New Delhi, Miami, and Philadelphia. *Games24x7 is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, disability status, or any other characteristic protected by the law.* General Accountabilities/Job<br>Responsibilities:• Design and development of sub seconds to single digit milliseconds latency, highly-concurrent distributed data analytics applications Work on Big Data technologies such as Hadoop, Kafka, Spark, Flink etc Work on massively parallel query engines like Ksql DB / K streams, Druid, Hive, Presto, Impala etc Develop, test and maintain optimal and scalable end-to-end data pipelines for Batch as well as Real Time data processing<br>• Ensure right stakeholders gets right information at right time The candidate should be comfortable around back-end coding languages, development frameworks and third-party libraries<br>• Participation in the requirements analysis, design, development and testing of applications<br>• Contributes to the development of project estimates, scheduling, and deliverables<br>• The candidate is expected to write code, code review, unit testing and deployment<br>• Design server-side architecture Write effective APIs Collaborate with peer data engineers and platform architects on various technical projects to maintain and enhance data analytics solutions Test software to ensure responsiveness and efficiency Create and maintain documentation of entire data landscape There is also the opportunity to mentor and guide junior team members in excelling at their jobs<br>• Mandatory Requirements: BE/B<br>• Tech in Computer Science/IT7+ Years of experience in Big Data technologies like Hadoop, Spark, Flink, Kafka, Ksql DB, Druid, Aerospike, Hive, Presto etc<br>• 3+ Years of experience in software development, with emphasis on JAVA/J2EE Server side programming, Hibernate, Spring, Spring Boot<br>• Hands on experience of working on low latency (sub seconds to single digit millisecond latencies), highly concurrent data serving distributed applications Extensive experience in end-to-end development and maintenance of batch data pipeline and near real time Streaming data pipeline<br>• Experience in Distributed caching architecture systems and In-memory data structure store’s like Redis, Memcached etc Knowledge of probabilistic data structures & algorithms such has Bloom filters, Cuckoo filters, Hyper Log Log etc Hands-on experience of REST / Web Services In-depth understanding of JVM metrics, Garbage collection and performance tuning Hands on experience in messaging frameworks like Kafka, Kinesis or Rabbit MQHands-on experience in Hadoop, Spark, Presto, Hive, Sqoop,Map Reduce<br>• , etc Hands-on experience of Java, Python, Linux and shell scripting<br>• Hands-on experience in Spark SQL, Hive QL and SQL<br>• Experience with integration of data from multiple data sources (Kafka, Mongo DB, Mysql, Cassandra, 3rd Party APIs etc)Should have knowledge of AWS services such as S3, EC2, Auto Scaling, Load Balancer, Lambda, EMR, Athena, Glue, Redshift etc<br>• Deep understanding of the Hadoop ecosystem and strong conceptual knowledge in Hadoop architecture components Strong experience with Data warehousing and Data modeling Capable of processing large sets of structured, semi-structured and unstructured data<br>• Understand the business requirements and build the Big Data Lake based on Big Data technologies<br>• Developing the data lake for real time data streaming from multiple data sources<br>• Working on and leading Proof of Concept (Po C) projects in the big data space<br>• Designing data pipelines to process any size and any file format<br>Qualifications:• </p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    