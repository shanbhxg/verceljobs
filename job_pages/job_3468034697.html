
<!DOCTYPE html>
<html>
<head>
    <title>Data Analyst</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Games24x7 is looking for Data Analyst!</h1>
            <h2>Bengaluru, Karnataka</h2>
        </header>
        <main>
            <p id="jobDescription">About the job IntroductionGames24x7 is India’s leading and most valuable multi-gaming unicorn. We’re a full-stack gaming company, offering awesome game playing experiences to over 100 million players through our products - RummyCircle, India’s first and largest online rummy platform, My11Circle, the country’s fastest growing fantasy sports platform, and U Games, a cutting-edge gaming studio making casual games in India for players across the globe. A pioneer in the online skill gaming industry in India, Games24x7 was founded in 2006 when two New York University trained economists Bhavin Pandya, and Trivikraman Thampy met at the computer lab and discovered their shared passion for online games. We’ve always been a technology company at heart, and over the last decade and a half, we’ve built the organisation on a strong foundation of ‘the science of gaming’, leveraging behavioural science, artificial intelligence, and machine learning to provide immersive and hyper-personalised gaming experiences to each of our players. Backed by marquee investors including Tiger Global Management, The Raine Group, and Malabar Investment Advisors, Games24x7 is leading the charge in India’s gaming revolution, constantly innovating and offering novel entertainment to players! Our 700 passionate teammates create their magic from our offices in Mumbai, Bengaluru, New Delhi, Miami, and Philadelphia. *Games24x7 is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, disability status, or any other characteristic protected by the law.* General Accountabilities/Job ResponsibilitiesDesign and development of sub seconds to single digit milliseconds latency, highly-concurrent distributed data analytics applicationsWork on Big Data technologies such as Hadoop, Kafka, Spark, Flink etcWork on massively parallel query engines like KsqlDB / K streams, Druid, Hive, Presto, Impala etcDevelop, test and maintain optimal and scalable end-to-end data pipelines for Batch as well as Real Time data processing.Ensure right stakeholders gets right information at right timeThe candidate should be comfortable around back-end coding languages, development frameworks and third-party libraries.Participation in the requirements analysis, design, development and testing of applications.Contributes to the development of project estimates, scheduling, and deliverables.The candidate is expected to write code, code review, unit testing and deployment.Design server-side architectureWrite effective APIsCollaborate with peer data engineers and platform architects on various technical projects to maintain and enhance data analytics solutionsTest software to ensure responsiveness and efficiencyCreate and maintain documentation of entire data landscapeThere is also the opportunity to mentor and guide junior team members in excelling at their jobs. Mandatory Requirements: BE/B.Tech in Computer Science/IT7+ Years of experience in Big Data technologies like Hadoop, Spark, Flink, Kafka, KsqlDB, Druid, Aerospike, Hive, Presto etc.3+ Years of experience in software development, with emphasis on JAVA/J2EE Server side programming, Hibernate, Spring, Spring Boot.Hands on experience of working on low latency (sub seconds to single digit millisecond latencies), highly concurrent data serving distributed applicationsExtensive experience in end-to-end development and maintenance of batch data pipeline and near real time Streaming data pipeline.Experience in Distributed caching architecture systems and In-memory data structure store’s like Redis, Memcached etcKnowledge of probabilistic data structures & algorithms such has Bloom filters, Cuckoo filters, HyperLogLog etcHands-on experience of REST / Web ServicesIn-depth understanding of JVM metrics, Garbage collection and performance tuningHands on experience in messaging frameworks like Kafka, Kinesis or RabbitMQHands-on experience in Hadoop, Spark, Presto, Hive, Sqoop,MapReduce., etcHands-on experience of Java, Python, Linux and shell scripting.Hands-on experience in SparkSQL, HiveQL and SQL.Experience with integration of data from multiple data sources (Kafka, MongoDB, Mysql, Cassandra, 3rd Party APIs etc)Should have knowledge of AWS services such as S3, EC2, AutoScaling, Load Balancer, Lambda, EMR, Athena, Glue, Redshift etc.Deep understanding of the Hadoop ecosystem and strong conceptual knowledge in Hadoop architecture componentsStrong experience with Data warehousing and Data modelingCapable of processing large sets of structured, semi-structured and unstructured data.Understand the business requirements and build the Big Data Lake based on Big Data technologies.Developing the data lake for real time data streaming from multiple data sources.Working on and leading Proof of Concept (PoC) projects in the big data space.Designing data pipelines to process any size and any file format.</p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    