
    <html>
    <head>
        <title>Data Analyst</title>
        <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <h1>Datavail</h1>
        <h2>Mumbai, Maharashtra</h2>
        <p>About the job Datavail helps customers manage their data and maximize IT efficiency through application of our deep expertise in databases  analytics  and applications. We are North America’s largest data integration and database administration company with more than 900  on payroll and core operations in four countries. Founded in 2007  Datavail is based in Broomfield  Colorado and supports mid-market and enterprise clients headquartered in North America. For more information  visit www.datavail.com. Position: AWS Data Engineer - Senior Technical Specialist Experience: 6+ yrs Qualifications: BE\B.Tech or EquivalentJob Location: Bangalore\Hyderabad\MumbaiKey Skills: AWS\Python\Pyspark\AWS Components (especially Glue  S3  RedShift  Lambda) Job Description: • The AWS ETL Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based projects.• The ideal candidate would also be responsible for developing and delivering AWS cloud solutions to meet today's high demand in areas such as AIML  IoT  advanced analytics  open source  enterprise collaboration  microservices  serverless  etc.• The AWS ETL Architect is responsible for delivering Cloud based Big Data and Analytical Solutions.• Responsibilities include evangelizing data on cloud solutions with business partners  leading Business and IT stakeholders through designing a robust  secure and optimized AWS architectures and ability to be hands-on delivering the target solution.• This role will work with business partners and leading internal data engineers in delivering big data solutions on cloud. The AWS ETL Architect will build ETL pipelines to ingest the data from heterogeneous sources into our system. Required Skills  Competencies  Authorities and Training Needs:• Understand the business requirements and convert them into Design• Create ETL jobs using Python/PySpark to fulfill the requirements• Create AWS Lambda for event based jobs• Automate the ETL process using AWS Step functions• Build data warehouse and load data into it• Communicate with customers and other stakeholders on the benefits of proposed solutions and framework• Communicating and documenting the project work• Need to be good team player and should be able help the team/customer on technical issues• Follow the customer release management process for release activities’• Expertise in ETL optimization  designing  coding  and tuning big data processes using Apache Spark or similar technologies.• Experience with building data pipelines and applications to stream and process datasets at low latencies.• Show efficiency in handling data - tracking data lineage  ensuring data quality  and improving discoverability of data.• Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines  knows how to optimize the distribution  partitioning  and MPP of high-level data structures• Knowledge of Engineering and Operational Excellence using standard methodologies.</p>
    </body>
    </html>
    