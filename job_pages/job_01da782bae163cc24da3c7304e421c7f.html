
<!DOCTYPE html>
<html>
<head>
    <title>Big Data Engineer / Architect</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Sacc, Inc. is looking for Big Data Engineer / Architect!</h1>
            <h2>Contract W2, C2H W2 | Boston, MA</h2>
            <h2>Hadoop, HDFS, Hive, Hbase, Map/Reduce, Python, Data Warehouse, ETL, SQL, Spark, Scala/Python, NoSQL, AWS, S3/Redshift</h2>
        </header>
        <main>
            <p id="jobDescription"><br>Additional Information:<br>The Big Data Engineering team is a group of engineers that are passionate about data. We are responsible for the complete data pipeline from processing raw data to applying analysis of the data. Our team is responsible for the designing and development of tools and systems for the entire lifecycle of data; data collection, data processing, monitoring, data quality and correction, data access platforms and more. We are looking for driven people to help us build the next generation platform for TV analytics. What is the job? • Architect a multi-tier data pipeline to feed data into an OLTP application in addition to an analytics environment • Design and build schemas to handle large scale interactive reporting • Design and build ETL/ELT process to move data through the data processing pipeline. • Manage complex data dependencies across datasets and incremental data loading workflows. • Be a fearless leader in championing smart design • Love scaling systems • Big Data Engineer -The Big Data Engineering team is a group of engineers that are passionate about data. We are responsible for the complete data pipeline from processing raw data to applying analysis of the data. Our team is responsible for the designing and development of tools and systems for the entire lifecycle of data; data collection, data processing, monitoring, data quality and correction, data access platforms and more. We are looking for a driven Big Data Engineer to help us build the next generation platform for TV analytics. About You: • A Bachelor's or Master's degree in computer science or software engineering • Hands on experience with Big Data stack of technologies –Hadoop, HDFS, Hive, Hbase • Strong expertise in Java Map/Reduce and Python • Experience with Amazon web services: On demand computing, S3, and EMR (Elastic Map Reduce) or equivalent cloud computing approaches • Hands on experience with Apache Spark using Scala or Python • Strong expertise in Data Warehousing and analytic architecture • Deep expertise in writing complex SQL and ETL batch processes • Experience working with large data volumes • The ability to get code in to production • A pro-active go get it attitude • The desire to automate the first time Nice to Have • Columnar and MPP database - Redshift, or similar technologies • Shell scripting experience • Strong knowledge of and experience with statistics • Ability to performance tune the SQL What Skills should you have? • Must have firm understanding of database systems – Data modeling, SQL query Processing and Transactions Know how to scale systems and make them fast! • Experience debugging and tracing SQL performance issues • Solid understanding of software development from design and architecture to production. • Large scale DW, MPP, Redshift, or similar technologies • Solid math skills. • The ability to present impromptu and via a whiteboard Bonus points for: • Experience with the Hadoop ecosystem (HBase/Hive/map reduce) • Experience working in AWS • Knowledge of Tableau • Experience with Redshift or Par Accel/Actian Matrix • Experience with Pentaho Kettle • Linux basics • Big, Big bonus if you know a programming language or two<br>Responsibilities:• <br>Qualifications:• </p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    