
    <html>
    <head>
        <title>Data Analyst</title>
        <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <h1>Neiman Marcus Group</h1>
        <h2>Bengaluru, Karnataka</h2>
        <p>About the job Neiman Marcus Group has an immediate opening for a Data Engineer. Data engineer will have the unique combination of business acumen needed to interface directly with key stakeholders to understand the problem along with the skills and vision to translate the need into a world-class technical solution using the latest technologies This person will be a hands-on role who is responsible for building data engineering solutions for NMG Enterprise using cloud-based data platform. Data engineer will provide day-to-day technical deliverables and participate in technical design  development and support for data engineering workloads. In this role  you need to be equally skilled with the whiteboard and the keyboard. Essential Duties & Responsibilities:Understand and Analyze data from multiple data sources and develop technology to integrate the enterprise data layerCreate robust and automated pipelines to ingest and process structured and unstructured data from source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolsetWork activity includes processing complex data sets  leveraging technologies used to process these disparate data sets and understanding the correlations as well as patterns that exist between these different data setsImplement orchestrations of data pipelines and environment using AirflowImplement custom applications using the Kinesis  Lambda and other AWS toolset as required to address streaming use casesImplement automation to optimize data platform compute and storage resourcesDevelop and enhance end to end monitoring capability of cloud data platformsParticipate in educating and cross training other team membersProvide regular updates to all relevant stakeholdersParticipate in daily scrum calls and provide clear visibility to work products Qualifications/Competencies:Bachelor’s degree with emphasis on Computer Science  Engineering  Information Systems  Mathematics  Statistics  or related discipline.6+ years of experience in the data engineering and analytic space5+ years of Python experience. Solid programing experience in Python - needs to be an expert in this 4/5 level. (Must have strong Python skills  along with lambdas and Airflow Dag processing.)8+ years of RDBMS concepts with strong data analysis and SQL experience3+ years of Linux OS command line tools and bash scripting proficiency1+ year of experience working on Big Data Processing Frameworks and ToolsExposure to software engineering such as parallel data processing  data flows  REST2+ years worked on real-time data capture  processing and storing using technologies like Kafka  AWS Kinesis.2+ year worked on AWS technology and servicesAPIs  JSON  XML  and micro service architecturesCertification –preferably AWS Certified Big Data or any other cloud data platforms  big data platforms Nice to have: Cloud data warehouse experience - Snowflake is a plusData Modeling experience a plus Essential Duties & Responsibilities:Understand and Analyze data from multiple data sources and develop technology to integrate the enterprise data layerCreate robust and automated pipelines to ingest and process structured and unstructured data from source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolsetWork activity includes processing complex data sets  leveraging technologies used to process these disparate data sets and understanding the correlations as well as patterns that exist between these different data setsImplement orchestrations of data pipelines and environment using AirflowImplement custom applications using the Kinesis  Lambda and other AWS toolset as required to address streaming use casesImplement automation to optimize data platform compute and storage resourcesDevelop and enhance end to end monitoring capability of cloud data platformsParticipate in educating and cross training other team membersProvide regular updates to all relevant stakeholdersParticipate in daily scrum calls and provide clear visibility to work products</p>
    </body>
    </html>
    