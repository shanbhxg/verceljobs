
<!DOCTYPE html>
<html>
<head>
    <title>Software Engineer in Qlty (Data Quality Engineer)</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Sumeru is looking for Software Engineer in Qlty (Data Quality Engineer)!</h1>
            <h2>Contract Independent, Contract W2, C2H W2 | Mountain View, CA</h2>
            <h2>Data, quality, hadoop/hive/bigdata, warehouse, qlikview/tableau/cassandra</h2>
        </header>
        <main>
            <p id="jobDescription"><br>Additional Information:<br>JOB TITLE: Software Engineer in Qlty (Data Quality Engineer)LOCATION: Mountain View, CADURATION: 12 Months ONLY W2 CANDIDATES CAN APPLY (H1B TRANSFER IS AN OPTION AS WELL)Job Description: Data is the catalyst that is powering newinnovative products that will improve the lives of our customers and help Intuit grow its business. Intuit's unique focus on delivering "Big Data for the Little Guy” empowers individuals and small businesses by allowing them to benefit from the power of their own data as well as the collective wisdom of millions of fellow Intuit customers. This means that small businesses now have access to insights that were once only available to big, multi-million dollar companies, and consumers are able to put their own data back to work for them. We're using data in groundbreaking ways to uncover customer insights, personalize customer experiences, and provide a unified customer view across all SBG products. The Data Engineering team at Intuit's Small Business Group (SBG) is looking for a Data Quality Engineer with a winning track record in Big Data, Data Warehousing, Visualization and Data Web Services.<br>Responsibilities:• :• Work with Data Engineers, Product Managers and Data Scientists to test data sets needed for deep customer insights and for building operational propensity models<br>• • Test data model appropriate for customer business use cases<br>• • Test data movement pipeline from source to Hive and Hive to Vertica• Test ETL code to populate the destination tables in Vertica<br>• • Work with BI developers to ensure that the data warehouse is providing the required data and the required performance<br>• • Serve as technical "go to” person for our core technologies – Hadoop, Vertica, Qlikview, Tableau, Cassandra and others<br>• • A commitment to writing understandable, maintainable, and reusable test code<br>• • An enormous sense of Quality ownership<br>Qualifications:• :• B<br>• S or M<br>• S in an engineering field (Computer Science, Computer Engineering, etc)• 5+ years of hands-on programming experience in one or more application or systems languages (Java, Python)• 5+ years experience testing data warehouse solutions using MPP platforms (Vertica or Red Shift)• 5+ years experience testing data pipelines from ingestion to delivery in critical data applications (Data Warehousing, Real-time Dashboarding)• Experience with one or more Hadoop Ecosystem of Tools (Spark, Hive, Pig, Oozie, Impala, Map Reduce, etc)<br>• • One or more database query languages desired: SQL, Pig, Hive QL, Vertica SQL, etc<br>• • ETL tools (Informatica or Kettle)<br>• • Experience with Database Performance testing and/or tuning is desired• Well versed in software and data design patterns<br>• • Willingness to learn new languages and methodologies• Strong understanding of different storage architectures and their appropriate application• Excellent interpersonal and communication skills, including business writing and presentations<br>• Ability to clearly communicate objectives, plans, status and results, focusing on a few critical key points</p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    