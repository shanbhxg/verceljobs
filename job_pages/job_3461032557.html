
<!DOCTYPE html>
<html>
<head>
    <title>Data Analyst</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Walmart is looking for Data Analyst!</h1>
            <h2>Bengaluru, Karnataka</h2>
        </header>
        <main>
            <p id="jobDescription">About the job About Global Tech in India Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption. This position is for the Common Data Platform (CDP) Team at IDC. At Walmart in Bangalore, you will work in a team of talented engineers to build Robust, Performant and Scalable Data Engineering solutions. This is an Individual Contributor role, where you will Architect, Design, Code, Review, Test, Deploy and Maintain Production systems in a team environment setup.Job Description (What you will do and What you will bring) Code and Develop interactive, user-friendly data engineering solutions using the latest frameworks which are open source and proprietary.Architect and Design Data Engineering Solution which focuses on Scalability, Reliability, Low-latency, and Cloud cost optimization.Design, build, test and deploy cutting edge solutions at scale, impacting millions of customers worldwide.Expected to drive and lead the project technicallyInteract with Walmart engineering teams across geographies to leverage expertise and contribute to the tech community.Engage with Product Management and Business to drive the requirements, set your priorities and deliver awesome solution.Foster motivating culture of openness, collaboration, and continuous improvement.Bring-in Best Engineering Practices and ideas which always challenges the current setup.Ensure business needs are being met using best Data Engineering practices.Participate in internal Hackathons and innovation challenges!Must be able to work effectively in a team setting as well as individually.Ability to communicate and collaborate with external teams and stakeholders.Must have growth mindset to learn and contribute across different modules or initiatives. About Team: Sam's Club is our membership warehouse club, a business model that provides our members with high-quality products at prices that are unrivalled by traditional retail. Sam's Club provides a carefully curated assortment of items, as well as developing and leading technologies and services such as Scan & Go, Club Pickup, and home delivery service in select markets. Sam's Club also provides travel, auto purchasing, pharmacy, optical, hearing aid centres, tire and battery centres, and a portfolio of business operations support services. Mandatory Technical Skills: Bachelor's or Master’s degree in Computer Science or related technical field10 to 15 years of total experience in developing Enterprise scale applicationsStrong in Architecture and Design for building enterprise scale Data Engineering SolutionsStrong hands-on coding experience in Spark, Python/ScalaStrong experience in Big Data, Hive, Data modelling, Spark streaming, Big Query and DatabricksStrong experience in Building Data Pipelines, Data Migration, Streaming Pipelines solutionsExperience to develop, implement and tune distributed data processing pipelines that process large volume of dataExpertise in writing complex, highly-optimized SQL queries across large data setsExperience of working in Git, Docker and Jenkins3-4 years’ of working experience in GCP/Azure/AWS cloud platformsExperience of working in Docker, Kubernetes and JenkinsDesirable skills:Delivering high quality data solutions that are testable and adhere to SLAsWorking knowledge of job scheduling tools like Airflow etc.Knowledge of Open Source Tools – Kubernetes, Kafka, Elastic Search, Kibana, AI/ML exposureGood to have exposure on Data Governance related projects</p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    