
<!DOCTYPE html>
<html>
<head>
    <title>Big Data architect</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Wilco Source, LLC is looking for Big Data architect!</h1>
            <h2>Full Time, Contract Corp-To-Corp, Contract W2, C2H Corp-To-Corp, C2H W2 | San Francisco, CA</h2>
            <h2>Big Data, Hadoop, Scala, Python, Oracle, ETL, RESTful API,</h2>
        </header>
        <main>
            <p id="jobDescription"><br>Additional Information:<br>*Its mandatory and must share with every submissions H1B copy and Photo ID, consultant/employer contact details, Rate, you can also send it to siva@wilcosource. com PNo 408-709-1675. Job Title: Big Data architect Location: SFO, CARate:$62Duration: 6+ Months Start Date:immidiatly Work Status: H1B, GC and Citizens Experience: 8+MOI: Telephonic & F2F or Skype. C2C Job description The Technical Architect will also have experience in Oracle and ETL tool like Ab Initio to build data marts/ data warehouses. He/She will be involved in designing a transient data mart using Oracle/Ab Initio until the long-term strategic Hadoop based data platform can replace it. The technical architect will have experience in RESTful API based web services. He/She will be involved in designing data distribution channels for downstream consumer applications. Qualifications: Minimum 8 years of experience with 3+ years of development experience in Big Data technologies like Hadoop and Scala / Python.· Ability to own and establish physical Architecture for Big Data platform.· Ability to design and support development of a data platform for data processing (data ingestion and data transformation) and data repository using Big Data Technologies like Hadoop stack including HDFS cluster, Map Reduce, Spark, Scala, Hive and Impala.· Past experience to build proof of concepts using Big Data technologies to test various use cases.· Ability to support logical data model design and convert it into physical data model.· Ability to design and support development of a data mart using Oracle and Ab Initio ETL platform.· Ability to design and support RESTful API based web services for data distribution to downstream applications.· Past experience working with best practices/standards for Big data platform and web services.· Past experience in translating functional and technical requirements into detail desig<br>Responsibilities:• <br>Qualifications:• : Minimum 8 years of experience with 3+ years of development experience in Big Data technologies like Hadoop and Scala / Python<br>• · Ability to own and establish physical Architecture for Big Data platform<br>• · Ability to design and support development of a data platform for data processing (data ingestion and data transformation) and data repository using Big Data Technologies like Hadoop stack including HDFS cluster, Map Reduce, Spark, Scala, Hive and Impala<br>• · Past experience to build proof of concepts using Big Data technologies to test various use cases<br>• · Ability to support logical data model design and convert it into physical data model<br>• · Ability to design and support development of a data mart using Oracle and Ab Initio ETL platform<br>• · Ability to design and support RESTful API based web services for data distribution to downstream applications<br>• · Past experience working with best practices/standards for Big data platform and web services<br>• · Past experience in translating functional and technical requirements into detail design</p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    