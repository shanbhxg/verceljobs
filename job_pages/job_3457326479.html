
<!DOCTYPE html>
<html>
<head>
    <title>Data Analyst</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Innovation Hacks AI is looking for Data Analyst!</h1>
            <h2>Bengaluru, Karnataka</h2>
        </header>
        <main>
            <p id="jobDescription">About the job Position: Azure Data EngineerYears of experience: 4+Location: Bangalore (Onsite)Role type: Contract basis Note: We are open to hire bench resources via vendor Requirements Design and architect data flows, data management in Hadoop or Cloud environment which are scalable, repeatable and eliminate time consuming steps Drive automation and efficiency in data ingestion, data movement and data access workflows by innovation and collaboration Design and develop data management and data persistence solutions for various use cases leveraging relations, non-relational databases and enhancing our data processing capabilities Work with product team to implement new modules, maintain and release production pipelines in timely and responsible manner Ability to think and perform data engineering workstreams with a product Skills and Qualifications: Â· Bachelor or Master's degree in Computer Science, Information Systems or equivalent field At least 3+ years of experience in building data flows and data management on modern big data tech stack Collaborating efforts with other technology teams to extract, transform, and load data from a wide variety of data sources using big data technologies. Collaborate with data scientists to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering, and machine learning Experience in one or more programming languages like PySpark , python and moderate knowledge on shell scripting. Strong understanding of distributed storage and compute (Hive and Spark) Should be comfortable in working within Microsoft Azure on services like Azure Data Factory, Azure Synapse, Azure DevOps, Azure DataBricks etc. Expertise in using query languages such as SQL, No-SQL, Hive and SparkSQL. Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for various business stakeholders within Landmark Group Identify the bottlenecks in the existing data pipelines, propose and lead the efforts to optimize those pipelines. Estimate and build data infrastructure to host and process large datasets. Strong experience in using ETL framework (eg. Airflow, Oozie, Jenkins etc.) to build and deploy production-quality ETL pipelines Experience in ingesting and transforming structured and unstructured data from internal and third-party sources into dimensional models Knowledge of data structures and distributed computing. Should be comfortable in manipulation and analysis of high-volume data from variety of internal and third-party sources Experience in building stream processing jobs on Apache Spark or similar steaming analytics technology Experience in debugging production issues, providing root cause and implementing mitigation plan Understanding of basics of machine learning would be an added advantage Open to learn and implement new technologies and perform POC to explore best solution for the problem statement Strong sense of urgency, learning appetite and commitment</p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    