
<!DOCTYPE html>
<html>
<head>
    <title>DevOps Engineer</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Anika Technologies is looking for DevOps Engineer!</h1>
            <h2>Remote | Contract | 0 applicants | Not Avilable</h2>
            <h2>Bengaluru, Karnataka</h2>
        </header>
        <main>
            <p id="jobDescription"><br>Additional Information:<br>JOB DESCRIPTION DATA ARCHITECT (Databricks & Pre Sales) Overview: As a Data Architect you will work with multiple teams to deliver solutions on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure Synapse Analytics and other Big Data related technologies). In addition to building the next generation of application data platforms (not infrastructure) and/or improving recent implementations. Experience in Databricks and Spark on other cloud platforms like AWS and GCP is also relevant.<br>Responsibilities:• : Defines, designs, develops and test software components/applications using Microsoft Azure- Databricks, ADF, ADL, Hive, Python, Databricks, Spark Sql, Py Spark<br>• Expertise in Azure Data Bricks, ADF, ADL, Hive, Python, Spark, Py Spark Strong T-SQL skills with experience in Azure SQL DW, Redshift, Big Query Experience handling Structured and unstructured datasets Experience in Data Modeling and Advanced SQL techniques Experience implementing Azure Data Factory, AWS Glue or any other data orchestration tool using latest technologies and techniques<br>• Good exposure in Application Development<br>• The candidate should work independently with minimal supervision<br>• Experience in Pre-Sales<br>• Must Have: Pre- Sales experience is a must<br>• Hands on experience with distributed computing framework like Data Bricks, Hadoop, Hive, Spark-Ecosystem (Spark Core, Py Spark, Spark Streaming), Spark SQL Willing to work with product teams to best optimize product features/functions<br>• Experience on Batch workloads and real time streaming with high volume data frequency Performance optimization on Spark workloads Environment setup, user management, Authentication and cluster management on Databricks Professional curiosity and the ability to enable yourself in new technologies and tasks<br>• Good understanding of SQL and a good grasp of relational and analytical database management theory and practice<br>• Good To Have: Hands on experience with distributed computing framework like Data Bricks, Hadoop, Hive, Experience with Databricks migration from On-preimse to Cloud OR Cloud to Cloud Migration of ETL workloads from Vanilla Spark implementations to Databricks Experience on Databricks ML will be a plus Migration from Spark 2<br>• 0 to Spark 3<br>• 0 Certifications : Databricks Solution Architect Essentials badge Databricks Developer Essentials Apache Spark Programming with Databricks Data Engineering with Databricks Lakehouse with Delta Lake Deep Dive Fundamentals of Unified Data Analytics with Databricks Key Skills: Python, SQL, Pyspark, Hive, Sqoop, YARN, Spark Ecosystem, Databricks and Azure Data Factory, Data Modelling, ETL Methodology, Azure Synapse Analytics, HDInsights, Hadoop<br>Qualifications:• </p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
    