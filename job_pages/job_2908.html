
    <html>
    <head>
        <title>Data Analyst</title>
        <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <h1>GEP Worldwide</h1>
        <h2>Navi Mumbai, Maharashtra</h2>
        <p>About the job GEP is a diverse  creative team of people passionate about procurement. We invest ourselves entirely in our client’s success  creating strong collaborative relationships that deliver extraordinary value year after year. Our clients include market global leaders with far-flung international operations  Fortune 500 and Global 2000 enterprises  leading government and public institutions. We deliver practical  effective services and software that enable procurement leaders to maximise their impact on business operations  strategy and financial performance. That’s just some of the things that we do in our quest to build a beautiful company  enjoy the journey and make a difference. GEP is a place where individuality is prized  and talent respected. We’re focused on what is real and effective. GEP is where good ideas and great people are recognized  results matter  and ability and hard work drive achievements. We’re a learning organization  actively looking for people to help shape  grow and continually improve us. Are you one of us? GEP is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race  ethnicity  color  national origin  religion  sex  disability status  or any other characteristics protected by law. We are committed to hiring and valuing a global diverse work team. For more information please visit us on GEP.com or check us out on LinkedIn.com. What You Will Do Build processes supporting data transformation  data structures  metadata  dependency and workload management Create CI/CD  unit testing framework/automation  data pipeline orchestration What You Should Bring Strong analytical skills related to working with unstructured datasets. Advanced working SQL knowledge and experience working with relational databases  query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ‘big data’ data pipelines  architectures and data sets. Build processes supporting data transformation  data structures  metadata  dependency and workload management. Experience in manipulating  processing and extracting value from large disconnected datasets. Working knowledge of message queuing  stream processing  and scalable ‘big data’ stores such as Kafka  Azure Event Hub/Grid  RabbitMQ  AWS Kinesis  Apache Pulsar Strong project management and collaboration skills. Experience supporting and working with cross-functional teams in a dynamic environment. Experience with big data tools: Hadoop  Hive  Spark  Kafka  etc. Experience with distributed file systems such as HDFS  GFS  Azure blob storage (ADLS)  AWS S3 Experience with both relational SQL and NoSQL databases such as SQL Server/Oracle/MySQL  Cassandra  HBase  MongoDB  Elastic Search Experience with object-oriented/object function scripting languages: Python  Java  Scala  R etc. Should have knowledge of CI/CD  unit testing framework/automation  data pipeline orchestration Cloud exposure - Azure  AWS  GCP (Experience on one cloud is a must) Additionally any cloud certifications – Azure  AWS  etc (this will be a plus) Experience with agile development (this will be a plus) Data Science and/or MLOps (this will be a major plus)</p>
    </body>
    </html>
    